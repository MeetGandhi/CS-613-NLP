{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Meet\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import *\n",
    "import numpy as np\n",
    "from urllib import request\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from scipy.optimize import curve_fit\n",
    "from PyDictionary import PyDictionary\n",
    "from vocabulary.vocabulary import Vocabulary as vb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gzip\n",
    "# url = \"http://www.gutenberg.org/cache/epub/11/pg11.txt\"  # Corpus used: The Adventures of Sherlock Holmes\n",
    "# res = request.urlopen(url)\n",
    "# gzipFile = gzip.GzipFile(fileobj=res)\n",
    "# text = str(gzipFile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.gutenberg.org/cache/epub/1661/pg1661.txt\"  # Corpus used: The Adventures of Sherlock Holmes\n",
    "res = request.urlopen(url)\n",
    "text = res.read().decode('utf8')\n",
    "# text= text.lower()\n",
    "# loading the corpus from the Ebook itself hence NOTE that there might be some extra text in the corpus too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenize_list = sent_tokenize(text)\n",
    "word_tokenize_list = word_tokenize(text)\n",
    "#tweet_tokenizer = TweetTokenizer()\n",
    "#tweet_tokenizer_list = tweet_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "translate_table_1 = dict((ord(char), None) for char in string.punctuation)   \n",
    "translate_table_2 = dict((ord(char), \" \") for char in string.whitespace[1:])\n",
    "\n",
    "sent_tokenize_filtered_list_og = [(sent_tokenize_list[i].translate(translate_table_1)).translate(translate_table_2)\n",
    "                          for i in range(2,len(sent_tokenize_list)-117)]\n",
    "sent_tokenize_filtered_list_og = [\" SsSs \"+sent_tokenize_filtered_list_og[i]+\" EeEe \" for i in range(len(sent_tokenize_filtered_list_og))]\n",
    "# HERE SsSs is thee tag/marker used forrepresenting the start of the sentence instead of <s>. Similarly EeEe is \n",
    "# used to represent the end of the sentence instead of <e> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sent_tokenize_filtered_list, sent_tokenize_filtered_list_test = sklearn.model_selection.train_test_split(\n",
    "    sent_tokenize_filtered_list_og, test_size = 0.2)\n",
    "# used for parsing the entire dataset into training and test sets in the ratio of 80% to 20% respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5464"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_tokenize_filtered_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_text = \"\".join(sent_tokenize_filtered_list)\n",
    "# tokens = sorted(set(word_tokenize_list),key = word_tokenize_list.index)\n",
    "tokens = word_tokenize(mod_text)\n",
    "translate_table_3 = dict((ord(char), None) for char in string.punctuation[0:17] or string.punctuation[18] or string.punctuation[20:])\n",
    "tokens = [tokens[i].translate(translate_table_3) for i in range(len(tokens))]\n",
    "for i in range(len(tokens)):\n",
    "    if tokens[i].isalpha()==False:\n",
    "        tokens[i]=\"\"\n",
    "tokens = [value for value in tokens if value != \"\"]\n",
    "\n",
    "# above code used for filtering out punctuations and numbers and only accepts alphabets\n",
    "\n",
    "# for i in range(len(tokens)):\n",
    "#    if tokens[i].isalpha()==False:\n",
    "#        tokens[i]=\"\"\n",
    "# tokens = sorted(set(tokens),key = tokens.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "unigrams = list(nltk.ngrams(tokens, 1))\n",
    "bigrams = list(nltk.ngrams(tokens, 2))\n",
    "trigrams = list(nltk.ngrams(tokens, 3))\n",
    "quadgrams = list(nltk.ngrams(tokens, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_unigrams = Counter(unigrams)\n",
    "count_bigrams = Counter(bigrams)\n",
    "count_trigrams = Counter(trigrams)\n",
    "count_quadgrams = Counter(quadgrams)\n",
    "no_of_tokens = len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here training corpus is used for computing the MLE\n",
    "# MLE corresponding to the entire corpus has been computed in cell [32]\n",
    "prob_unigrams = dict()\n",
    "for i in range(len(unigrams)):\n",
    "    prob_unigrams[unigrams[i]] = count_unigrams[unigrams[i]]/no_of_tokens \n",
    "    \n",
    "prob_bigrams = dict()\n",
    "for i in range(len(bigrams)):\n",
    "    prob_bigrams[bigrams[i]] = count_bigrams[bigrams[i]]/count_unigrams[(bigrams[i][0],)] \n",
    "\n",
    "prob_trigrams = dict()\n",
    "for i in range(len(trigrams)):\n",
    "    prob_trigrams[trigrams[i]] = count_trigrams[trigrams[i]]/count_bigrams[(trigrams[i][0],trigrams[i][1])] \n",
    "    \n",
    "prob_quadgrams = dict()\n",
    "for i in range(len(quadgrams)):\n",
    "    prob_quadgrams[quadgrams[i]] = count_quadgrams[quadgrams[i]]/count_trigrams[(quadgrams[i][0],quadgrams[i][1],quadgrams[i][2])] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of possible Bigrams Order Important: 64056012\n",
      "Total Bigrams Order Not Important: 32028006\n",
      "Total no of possible Trigrams Order Important: 512576208024\n",
      "Total Trigrams Order Not Important: 85429368004\n",
      "Total no of possible Quadgrams Order Important: 4101122240400024\n",
      "Total Quadgrams Order Not Important: 170880093350001\n",
      "#Bigrams in the training set:42847\n",
      "#Trigrams in the training set:72205\n",
      "#Quadgrams in the training set:86314\n"
     ]
    }
   ],
   "source": [
    "# Here tokens contain just the training data and not the entire corpus hence n-grams also correspond to the training data and \n",
    "# not the entire corpus\n",
    "\n",
    "from math import factorial as fac\n",
    "\n",
    "len_types = len(set(tokens)) \n",
    "\n",
    "# from itertools import permutations, combinations\n",
    "\n",
    "# total_bigrams_ordered = len(list(permutations(list(set(tokens)),2)))\n",
    "# total_bigrams_order_not_imp = len(list(combinations(list(set(tokens)),2)))\n",
    "\n",
    "# total_trigrams_ordered = len(list(permutations(list(set(tokens)),3)))\n",
    "# total_trigrams_order_not_imp = len(list(combinations(list(set(tokens)),3)))\n",
    "\n",
    "# total_quadgrams_ordered = len(list(permutations(list(set(tokens)),4)))\n",
    "# total_quadgrams_order_not_imp = len(list(combinations(list(set(tokens)),4)))\n",
    "\n",
    "print(\"Total no of possible Bigrams Order Important: \"+str(int(fac(len_types)/fac(len_types-2))))\n",
    "print(\"Total Bigrams Order Not Important: \"+str(int(fac(len_types)/(fac(len_types-2)*fac(2)))))\n",
    "\n",
    "print(\"Total no of possible Trigrams Order Important: \"+str(int(fac(len_types)/fac(len_types-3))))\n",
    "print(\"Total Trigrams Order Not Important: \"+str(int(fac(len_types)/(fac(len_types-3)*fac(3)))))\n",
    "\n",
    "print(\"Total no of possible Quadgrams Order Important: \"+str(int(fac(len_types)/fac(len_types-4))))\n",
    "print(\"Total Quadgrams Order Not Important: \"+str(int(fac(len_types)/(fac(len_types-4)*fac(4)))))\n",
    "\n",
    "print(\"#Bigrams in the training set:\"+str(len(set(bigrams))))\n",
    "print(\"#Trigrams in the training set:\"+str(len(set(trigrams))))\n",
    "print(\"#Quadgrams in the training set:\"+str(len(set(quadgrams))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Probability(sentence, model_name):  # in logspace\n",
    "    \n",
    "    sentence = ' SsSs '+sentence+' EeEe '\n",
    "    words = word_tokenize(sentence)\n",
    "    len_words = len(words)\n",
    "    \n",
    "    if model_name==unigrams:\n",
    "        result_prob = 0\n",
    "        for i in range(len(words)):\n",
    "            result_prob = result_prob + prob_unigrams[(words[i],)]\n",
    "    \n",
    "    if model_name==bigrams:\n",
    "        result_prob = 0 #prob_unigrams[(words[0],)]\n",
    "        for i in range(1,len(words)):\n",
    "            result_prob = result_prob + prob_bigrams[(words[i-1],words[i])]\n",
    "    \n",
    "    if model_name==trigrams:\n",
    "        result_prob = prob_bigrams[(words[0],words[1])] #prob_unigrams[(words[0],)] + prob_bigrams[(words[0],words[1])]\n",
    "        for i in range(2,len(words)):\n",
    "            result_prob = result_prob + prob_trigrams[(words[i-2],words[i-1],words[i])]\n",
    "            \n",
    "    if model_name==quadgrams:\n",
    "        result_prob = prob_bigrams[(words[0],words[1])] + prob_trigrams[(words[0],words[1],words[2])]#prob_unigrams[(words[0],)] + prob_bigrams[(words[0],words[1])] + prob_trigrams[(words[0],words[1],words[2])]\n",
    "        for i in range(3,len(words)):\n",
    "            result_prob = result_prob + prob_quadgrams[(words[i-3],words[i-2],words[i-1],words[i])]\n",
    "    \n",
    "    return result_prob\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generator(model_name):\n",
    "    \n",
    "    import operator\n",
    "    import random\n",
    "    prob_unigrams_sorted = dict(sorted(prob_unigrams.items(), key=operator.itemgetter(1))[::-1])\n",
    "    prob_bigrams_sorted = dict(sorted(prob_bigrams.items(), key=operator.itemgetter(1))[::-1])\n",
    "    prob_trigrams_sorted = dict(sorted(prob_trigrams.items(), key=operator.itemgetter(1))[::-1])\n",
    "    prob_quadgrams_sorted = dict(sorted(prob_quadgrams.items(), key=operator.itemgetter(1))[::-1])\n",
    "    \n",
    "    if model_name==bigrams:\n",
    "        sentence=[\"SsSs\"]\n",
    "        word=sentence[0]\n",
    "        counter = 0\n",
    "        while word!=\"EeEe\" and counter<=25:\n",
    "            candidate_bigrams = []\n",
    "            candidate_bigrams_keys = list(prob_bigrams_sorted.keys())\n",
    "            for i in range(len(candidate_bigrams_keys)):\n",
    "                if candidate_bigrams_keys[i][0]==word:\n",
    "                    candidate_bigrams = candidate_bigrams + [candidate_bigrams_keys[i]]\n",
    "            if word==sentence[0]:\n",
    "                word = random.choice(candidate_bigrams)[1]\n",
    "            else:\n",
    "#                result = random.choice(candidate_bigrams)\n",
    "                maximum = 0.0\n",
    "                for i in range(len(candidate_bigrams)):\n",
    "                    if prob_bigrams_sorted[candidate_bigrams[i]]>maximum:\n",
    "                        result=candidate_bigrams[i]\n",
    "                word = result[1]\n",
    "            sentence = sentence + [word]\n",
    "            counter = counter + 1\n",
    "            #print(sentence)\n",
    "        if counter>25:\n",
    "            sentence = sorted(set(sentence),key = sentence.index)\n",
    "        return \" \".join(sentence)\n",
    "    \n",
    "    if model_name==trigrams:\n",
    "        sentence=[\"SsSs\"]\n",
    "        word=sentence[0]\n",
    "        candidate_bigrams = []\n",
    "        candidate_bigrams_keys = list(prob_bigrams_sorted.keys())\n",
    "        for i in range(len(candidate_bigrams_keys)):\n",
    "            if candidate_bigrams_keys[i][0]==word:\n",
    "                candidate_bigrams = candidate_bigrams + [candidate_bigrams_keys[i]]\n",
    "        if word==sentence[0]:\n",
    "            word = random.choice(candidate_bigrams)[1]\n",
    "        sentence = sentence + [word]\n",
    "        while word!=\"EeEe\":\n",
    "            candidate_trigrams = []\n",
    "            candidate_trigrams_keys = list(prob_trigrams_sorted.keys())\n",
    "            for i in range(len(candidate_trigrams_keys)):\n",
    "                if candidate_trigrams_keys[i][0]==sentence[-2] and candidate_trigrams_keys[i][1]==sentence[-1]:\n",
    "                    candidate_trigrams = candidate_trigrams + [candidate_trigrams_keys[i]]\n",
    "#            result = random.choice(candidate_trigrams)\n",
    "            maximum = 0.0\n",
    "            for i in range(len(candidate_trigrams)):\n",
    "                if prob_trigrams_sorted[candidate_trigrams[i]]>maximum:\n",
    "                    result=candidate_trigrams[i]\n",
    "            word = result[2]\n",
    "            sentence = sentence + [word]\n",
    "        return \" \".join(sentence)\n",
    "    \n",
    "    if model_name==quadgrams:\n",
    "        sentence=[\"SsSs\"]\n",
    "        word=sentence[0]\n",
    "        candidate_trigrams = []\n",
    "        candidate_trigrams_keys = list(prob_trigrams_sorted.keys())\n",
    "        for i in range(len(candidate_trigrams_keys)):\n",
    "            if candidate_trigrams_keys[i][0]==word:\n",
    "                candidate_trigrams = candidate_trigrams + [candidate_trigrams_keys[i]]\n",
    "        if word==sentence[0]:\n",
    "            trigram = random.choice(candidate_trigrams)\n",
    "        sentence = sentence + [trigram[1]] + [trigram[2]]\n",
    "        while word!=\"EeEe\":\n",
    "            candidate_quadgrams = []\n",
    "            candidate_quadgrams_keys = list(prob_quadgrams_sorted.keys())\n",
    "            for i in range(len(candidate_quadgrams_keys)):\n",
    "                if candidate_quadgrams_keys[i][0]==sentence[-3] and candidate_quadgrams_keys[i][1]==sentence[-2] and candidate_quadgrams_keys[i][2]==sentence[-1]:\n",
    "                    candidate_quadgrams = candidate_quadgrams + [candidate_quadgrams_keys[i]]\n",
    "#            result = random.choice(candidate_quadgrams) \n",
    "            maximum = 0.0\n",
    "            for i in range(len(candidate_quadgrams)):\n",
    "                if prob_quadgrams_sorted[candidate_quadgrams[i]]>maximum:\n",
    "                    result=candidate_quadgrams[i]\n",
    "            word = result[3]\n",
    "            sentence = sentence + [word]\n",
    "        return \" \".join(sentence)\n",
    "    \n",
    "    if model_name==unigrams:\n",
    "        sentence=[\"SsSs\"]\n",
    "        word=sentence[0]\n",
    "        while word!=\"EeEe\":\n",
    "            candidate_unigrams_keys = list(prob_unigrams_sorted.keys())\n",
    "            result = random.choice(candidate_unigrams_keys)\n",
    "            word=result[0]\n",
    "            sentence=sentence + [word]\n",
    "        return \" \".join(sentence)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SsSs Within there glimmered little reputation such tricks with just treat myself into two hard black lines while away these signs that foul tongue over upon each'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Generator(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SsSs Because there are sentimental considerations in the history of the office after me EeEe'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Generator(trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SsSs The case is as plain as a pikestaff and the more ready to engage in an affair when there is some stiffness in the working of it and I want you to come tonight by the last train EeEe'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Generator(quadgrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_tokens = sorted(set(tokens),key = tokens.index)\n",
    "voc_len = len(uniq_tokens)\n",
    "count_bigrams_smoothed = dict() \n",
    "for (first,second) in prob_bigrams:\n",
    "    count_bigrams_smoothed[(first,second)] = ((count_bigrams[(first,second)] + 1) * count_unigrams[(first,)])/(count_unigrams[(first,)] + voc_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_bigrams_diff=dict()\n",
    "for k, v in count_bigrams.items():\n",
    "    count_bigrams_diff[k] = v - count_bigrams_smoothed.get(k, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(('EeEe', 'SsSs'), 3246.2420552420554),\n",
       "             (('SsSs', 'I'), 472.65518265518267),\n",
       "             (('of', 'the'), 445.86755098000395),\n",
       "             (('in', 'the'), 340.289050782087),\n",
       "             (('SsSs', 'It'), 198.68399168399168),\n",
       "             (('to', 'the'), 197.66128237959225),\n",
       "             (('I', 'have'), 181.94416047834892),\n",
       "             (('at', 'the'), 167.63275520893959),\n",
       "             (('that', 'I'), 166.30410547754315),\n",
       "             (('SsSs', 'The'), 158.86605286605288),\n",
       "             (('upon', 'the'), 151.8939828080229),\n",
       "             (('SsSs', 'He'), 141.63142263142265),\n",
       "             (('with', 'a'), 133.4041690793283),\n",
       "             (('it', 'EeEe'), 130.12596856320567),\n",
       "             (('SsSs', 'You'), 129.15117315117317),\n",
       "             (('It', 'is'), 128.53008870774394),\n",
       "             (('and', 'I'), 126.43231099824185),\n",
       "             (('to', 'be'), 119.61577858760958),\n",
       "             (('it', 'is'), 117.72216072614567),\n",
       "             (('I', 'was'), 113.2436107628508),\n",
       "             (('was', 'a'), 111.86794927853083),\n",
       "             (('said', 'he'), 111.37025580011898),\n",
       "             (('of', 'a'), 108.33993268659671),\n",
       "             (('that', 'he'), 107.96216100332298),\n",
       "             (('and', 'the'), 107.66927134205899),\n",
       "             (('I', 'am'), 106.29636416240717),\n",
       "             (('is', 'a'), 106.21251688428636),\n",
       "             (('it', 'was'), 106.20433916316139),\n",
       "             (('that', 'the'), 101.95637260156501),\n",
       "             (('It', 'was'), 100.70510668904339),\n",
       "             (('from', 'the'), 100.38910133843213),\n",
       "             (('I', 'had'), 97.033368695149),\n",
       "             (('with', 'the'), 95.40023161551824),\n",
       "             (('had', 'been'), 94.0440396587503),\n",
       "             (('SsSs', 'And'), 93.49331749331749),\n",
       "             (('into', 'the'), 93.46258668937827),\n",
       "             (('have', 'been'), 91.62190650779101),\n",
       "             (('in', 'a'), 88.18105849582173),\n",
       "             (('me', 'EeEe'), 86.58348040945994),\n",
       "             (('that', 'it'), 85.65494693965056),\n",
       "             (('him', 'EeEe'), 84.34275787708158),\n",
       "             (('of', 'his'), 83.77806374975252),\n",
       "             (('said', 'Holmes'), 82.8015466983938),\n",
       "             (('by', 'the'), 82.38509993943065),\n",
       "             (('on', 'the'), 82.26406193298658),\n",
       "             (('SsSs', 'But'), 82.2016632016632),\n",
       "             (('he', 'had'), 82.02163729645326),\n",
       "             (('you', 'EeEe'), 81.06486125041792),\n",
       "             (('to', 'me'), 78.62218063626514),\n",
       "             (('he', 'was'), 78.4508141869284),\n",
       "             (('for', 'the'), 77.69101123595506),\n",
       "             (('SsSs', 'There'), 77.44728244728245),\n",
       "             (('and', 'a'), 74.83395194373901),\n",
       "             (('in', 'his'), 74.46089565031069),\n",
       "             (('Holmes', 'EeEe'), 73.61599139476515),\n",
       "             (('which', 'I'), 73.37797653618307),\n",
       "             (('Sherlock', 'Holmes'), 73.31294874969052),\n",
       "             (('that', 'you'), 71.92743059277521),\n",
       "             (('as', 'I'), 66.744),\n",
       "             (('SsSs', 'Then'), 64.96703296703296),\n",
       "             (('you', 'have'), 64.11668338348379),\n",
       "             (('a', 'very'), 62.00478278198486),\n",
       "             (('one', 'of'), 61.962604380975435),\n",
       "             (('out', 'of'), 61.01646489104116),\n",
       "             (('I', 'should'), 59.98138682611631),\n",
       "             (('I', 'could'), 59.20947053717813),\n",
       "             (('a', 'little'), 58.81466719808689),\n",
       "             (('I', 'shall'), 58.43755424823995),\n",
       "             (('think', 'that'), 57.057044504548806),\n",
       "             (('SsSs', 'What'), 56.64686664686665),\n",
       "             (('with', 'his'), 55.542443543717425),\n",
       "             (('and', 'that'), 55.28911896854854),\n",
       "             (('SsSs', 'We'), 54.86397386397386),\n",
       "             (('would', 'be'), 54.22004357298475),\n",
       "             (('but', 'I'), 53.43598615916955),\n",
       "             (('that', 'she'), 53.05209561582163),\n",
       "             (('as', 'to'), 52.824),\n",
       "             (('has', 'been'), 52.51568198395332),\n",
       "             (('could', 'not'), 52.46368715083799),\n",
       "             (('then', 'EeEe'), 51.767183494078864),\n",
       "             (('there', 'was'), 51.17479478512796),\n",
       "             (('Mr', 'Holmes'), 50.56965718453684),\n",
       "             (('to', 'my'), 50.24199743918054),\n",
       "             (('a', 'man'), 50.04184934236748),\n",
       "             (('I', 'can'), 49.17455878098177),\n",
       "             (('SsSs', 'Yes'), 48.92099792099792),\n",
       "             (('of', 'my'), 48.9160562264898),\n",
       "             (('in', 'my'), 47.87808013713306),\n",
       "             (('I', 'think'), 46.85880991416723),\n",
       "             (('is', 'the'), 46.75011256190905),\n",
       "             (('SsSs', 'A'), 46.543807543807546),\n",
       "             (('which', 'he'), 46.4159600418167),\n",
       "             (('he', 'EeEe'), 46.31340620120455),\n",
       "             (('me', 'to'), 46.08789269325803),\n",
       "             (('SsSs', 'She'), 45.35521235521236),\n",
       "             (('SsSs', 'No'), 44.76091476091476),\n",
       "             (('SsSs', 'That'), 44.76091476091476),\n",
       "             (('There', 'is'), 44.242565740968296),\n",
       "             (('do', 'not'), 43.785792482666345),\n",
       "             (('the', 'door'), 43.744491943439655),\n",
       "             (('so', 'EeEe'), 43.17324535092981),\n",
       "             (('to', 'his'), 43.14695163990939),\n",
       "             (('the', 'matter'), 43.08648470897731),\n",
       "             (('You', 'have'), 41.833373874969595),\n",
       "             (('SsSs', 'Well'), 41.789426789426784),\n",
       "             (('SsSs', 'As'), 41.789426789426784),\n",
       "             (('SsSs', 'This'), 41.1951291951292),\n",
       "             (('that', 'we'), 41.04051881230571),\n",
       "             (('when', 'I'), 40.91596638655462),\n",
       "             (('as', 'a'), 40.76),\n",
       "             (('such', 'a'), 40.54325259515571),\n",
       "             (('was', 'the'), 40.122431132487975),\n",
       "             (('said', 'I'), 39.94848304580607),\n",
       "             (('a', 'few'), 39.673973694699086),\n",
       "             (('there', 'is'), 39.580395943988414),\n",
       "             (('can', 'not'), 39.171869261843554),\n",
       "             (('SsSs', 'Oh'), 38.81793881793882),\n",
       "             (('seemed', 'to'), 38.731943410275505),\n",
       "             (('all', 'the'), 38.62854727689893),\n",
       "             (('that', 'there'), 38.46660949726659),\n",
       "             (('if', 'you'), 38.27861612072138),\n",
       "             (('SsSs', 'My'), 38.22364122364122),\n",
       "             (('which', 'was'), 38.04843768149611),\n",
       "             (('as', 'he'), 37.976),\n",
       "             (('no', 'doubt'), 37.81668283220175),\n",
       "             (('to', 'do'), 37.628582684920715),\n",
       "             (('must', 'be'), 37.40501968503937),\n",
       "             (('should', 'be'), 37.263790144643295),\n",
       "             (('is', 'not'), 36.83971184151283),\n",
       "             (('That', 'is'), 36.61929499072356),\n",
       "             (('and', 'then'), 36.526079312365695),\n",
       "             (('the', 'other'), 36.506412364353835),\n",
       "             (('did', 'not'), 36.484841015528716),\n",
       "             (('to', 'see'), 36.051905840638234),\n",
       "             (('into', 'a'), 36.00596179583891),\n",
       "             (('He', 'was'), 35.889266221952695),\n",
       "             (('and', 'he'), 35.744285993358076),\n",
       "             (('have', 'a'), 35.68194317140238),\n",
       "             (('to', 'you'), 35.263567418497),\n",
       "             (('down', 'the'), 35.19506233194818),\n",
       "             (('when', 'he'), 35.0672268907563),\n",
       "             (('be', 'a'), 34.77837901423362),\n",
       "             (('of', 'it'), 34.65432587606415),\n",
       "             (('to', 'a'), 34.47522899635575),\n",
       "             (('There', 'was'), 34.40722536249693),\n",
       "             (('SsSs', 'If'), 34.06355806355806),\n",
       "             (('do', 'you'), 34.049750638608444),\n",
       "             (('the', 'room'), 33.87438342650444),\n",
       "             (('you', 'will'), 33.788365095285855),\n",
       "             (('to', 'have'), 33.686890574214516),\n",
       "             (('through', 'the'), 33.679376083188906),\n",
       "             (('as', 'we'), 33.336),\n",
       "             (('was', 'not'), 33.12286838653257),\n",
       "             (('he', 'is'), 32.92281954048628),\n",
       "             (('and', 'his'), 32.6171127173276),\n",
       "             (('the', 'house'), 32.558368957579745),\n",
       "             (('up', 'and'), 31.966202301635374),\n",
       "             (('she', 'had'), 31.89845261121857),\n",
       "             (('so', 'that'), 31.6497900419916),\n",
       "             (('you', 'are'), 31.112337011033098),\n",
       "             (('SsSs', 'Now'), 31.09207009207009),\n",
       "             (('SsSs', 'When'), 31.09207009207009),\n",
       "             (('St', 'Simon'), 30.718637770897832),\n",
       "             (('we', 'shall'), 30.71613832853026),\n",
       "             (('from', 'his'), 30.56453154875717),\n",
       "             (('upon', 'his'), 30.534383954154727),\n",
       "             (('may', 'be'), 30.392082363034685),\n",
       "             (('my', 'own'), 30.25844245348036),\n",
       "             (('it', 'would'), 30.00951959264999),\n",
       "             (('I', 'do'), 29.876651557527246),\n",
       "             (('to', 'him'), 29.745198463508324),\n",
       "             (('but', 'the'), 29.560553633217992),\n",
       "             (('if', 'I'), 29.440927493559073),\n",
       "             (('his', 'hand'), 28.727855936972425),\n",
       "             (('his', 'eyes'), 28.727855936972425),\n",
       "             (('he', 'has'), 28.45929065358019),\n",
       "             (('me', 'that'), 28.194493469819978),\n",
       "             (('man', 'EeEe'), 28.12310491206792),\n",
       "             (('man', 'who'), 28.12310491206792),\n",
       "             (('SsSs', 'His'), 28.12058212058212),\n",
       "             (('for', 'a'), 28.040730337078653),\n",
       "             (('and', 'we'), 27.92635280328189),\n",
       "             (('his', 'face'), 27.827011817670233),\n",
       "             (('as', 'the'), 27.768),\n",
       "             (('came', 'to'), 27.61390532544379),\n",
       "             (('room', 'EeEe'), 27.606852353956125),\n",
       "             (('about', 'the'), 27.575156961713652),\n",
       "             (('see', 'that'), 27.3760391198044),\n",
       "             (('that', 'EeEe'), 27.31300246543038),\n",
       "             (('who', 'had'), 27.303377636873552),\n",
       "             (('in', 'this'), 27.29783586886651),\n",
       "             (('in', 'which'), 27.29783586886651),\n",
       "             (('what', 'I'), 27.203645200486026),\n",
       "             (('up', 'in'), 27.11823137492429),\n",
       "             (('sir', 'EeEe'), 26.81236038719285),\n",
       "             (('I', 'will'), 26.788986401774523),\n",
       "             (('I', 'thought'), 26.788986401774523),\n",
       "             (('asked', 'EeEe'), 26.74006683995544),\n",
       "             (('of', 'this'), 26.731142348049893),\n",
       "             (('had', 'a'), 26.68272999769426),\n",
       "             (('did', 'you'), 26.62040916933695),\n",
       "             (('them', 'EeEe'), 26.60339943342776),\n",
       "             (('must', 'have'), 26.572834645669293),\n",
       "             (('us', 'EeEe'), 26.511907684753254),\n",
       "             (('will', 'be'), 26.284149013878743),\n",
       "             (('for', 'I'), 26.167134831460675),\n",
       "             (('up', 'the'), 26.14863718958207),\n",
       "             (('up', 'to'), 26.14863718958207),\n",
       "             (('would', 'not'), 26.12563543936093),\n",
       "             (('a', 'small'), 26.115982463132724),\n",
       "             (('there', 'EeEe'), 26.053597295992276),\n",
       "             (('at', 'once'), 26.01850774065883),\n",
       "             (('him', 'to'), 25.849406972564992),\n",
       "             (('side', 'of'), 25.779182156133828),\n",
       "             (('SsSs', 'They'), 25.743391743391744),\n",
       "             (('matter', 'EeEe'), 25.647102342786685),\n",
       "             (('have', 'no'), 25.594408799266727),\n",
       "             (('and', 'there'), 25.580972846259037),\n",
       "             (('may', 'have'), 25.487069493810516),\n",
       "             (('be', 'the'), 25.363016115751087),\n",
       "             (('was', 'in'), 25.248360297332752),\n",
       "             (('SsSs', 'Holmes'), 25.14909414909415),\n",
       "             (('of', 'course'), 25.14650564244704),\n",
       "             (('she', 'was'), 25.1252417794971),\n",
       "             (('his', 'head'), 25.12447945976365),\n",
       "             (('all', 'that'), 25.096848206738315),\n",
       "             (('we', 'were'), 24.949567723342938),\n",
       "             (('you', 'know'), 24.868271481109996),\n",
       "             (('you', 'can'), 24.868271481109996),\n",
       "             (('you', 'to'), 24.868271481109996),\n",
       "             (('enough', 'to'), 24.84500745156483),\n",
       "             (('but', 'he'), 24.785467128027683),\n",
       "             (('house', 'EeEe'), 24.723609394313968),\n",
       "             (('it', 'and'), 24.693601948195706),\n",
       "             (('door', 'EeEe'), 24.650684087267347),\n",
       "             (('over', 'the'), 24.559322033898304),\n",
       "             (('SsSs', 'In'), 24.554796554796553),\n",
       "             (('I', 'saw'), 24.473237534959978),\n",
       "             (('I', 'would'), 24.473237534959978),\n",
       "             (('was', 'no'), 24.373414954088325),\n",
       "             (('by', 'a'), 24.209448818897638),\n",
       "             (('would', 'have'), 24.18809005083515),\n",
       "             (('and', 'to'), 24.0173862082438),\n",
       "             (('the', 'same'), 24.00427490956922),\n",
       "             (('he', 'would'), 23.995761766674104),\n",
       "             (('Lord', 'St'), 23.891155616370195),\n",
       "             (('This', 'is'), 23.783254892246717),\n",
       "             (('might', 'have'), 23.69760552949889),\n",
       "             (('do', 'EeEe'), 23.340104610144753),\n",
       "             (('could', 'see'), 23.30167597765363),\n",
       "             (('which', 'is'), 23.172842374259496),\n",
       "             (('he', 'said'), 23.103055989292884),\n",
       "             (('he', 'could'), 23.103055989292884),\n",
       "             (('him', 'and'), 22.97268479693303),\n",
       "             (('I', 'know'), 22.929404957083616),\n",
       "             (('able', 'to'), 22.92527089301283),\n",
       "             (('from', 'a'), 22.912523900573614),\n",
       "             (('my', 'friend'), 22.903514817367334),\n",
       "             (('upon', 'my'), 22.889684813753583),\n",
       "             (('against', 'the'), 22.877688004972033),\n",
       "             (('say', 'that'), 22.791924696556848),\n",
       "             (('to', 'her'), 22.650152664237172),\n",
       "             (('shall', 'be'), 22.587426326129666),\n",
       "             (('down', 'to'), 22.477878269371793),\n",
       "             (('who', 'is'), 22.42348494086087),\n",
       "             (('his', 'hands'), 22.421947101857064),\n",
       "             (('for', 'me'), 22.41994382022472),\n",
       "             (('when', 'the'), 22.394957983193276),\n",
       "             (('You', 'will'), 22.36365847725614),\n",
       "             (('at', 'last'), 22.291817017809336),\n",
       "             (('which', 'you'), 22.243117667557208),\n",
       "             (('that', 'my'), 22.165183835352128),\n",
       "             (('that', 'this'), 22.165183835352128),\n",
       "             (('I', 'found'), 22.157488668145433),\n",
       "             (('I', 'must'), 22.157488668145433),\n",
       "             (('On', 'the'), 21.90841214534594),\n",
       "             (('Baker', 'Street'), 21.90841214534594),\n",
       "             (('night', 'EeEe'), 21.82603843769374),\n",
       "             (('might', 'be'), 21.721797087138977),\n",
       "             (('back', 'to'), 21.699383477188658),\n",
       "             (('and', 'it'), 21.672006251220942),\n",
       "             (('come', 'to'), 21.668636867380865),\n",
       "             (('And', 'now'), 21.554765988728253),\n",
       "             (('for', 'it'), 21.48314606741573),\n",
       "             (('You', 'are'), 21.390172707370468),\n",
       "             (('I', 'knew'), 21.385572379207254),\n",
       "             (('at', 'a'), 21.360144337096962),\n",
       "             (('you', 'would'), 21.30023403543965),\n",
       "             (('there', 'are'), 21.222597778850798),\n",
       "             (('to', 'go'), 21.073475819954695),\n",
       "             (('have', 'the'), 21.00916590284143),\n",
       "             (('SsSs', 'How'), 20.98901098901099),\n",
       "             (('SsSs', 'said'), 20.98901098901099),\n",
       "             (('Do', 'you'), 20.923306772908365),\n",
       "             (('wish', 'to'), 20.901492537313434),\n",
       "             (('and', 'as'), 20.890212932213323),\n",
       "             (('knew', 'that'), 20.879721669980118),\n",
       "             (('far', 'as'), 20.87700335445397),\n",
       "             (('If', 'you'), 20.841726618705035),\n",
       "             (('St', 'Clair'), 20.80656346749226),\n",
       "             (('tell', 'you'), 20.801163798440015),\n",
       "             (('Then', 'I'), 20.701750061621887),\n",
       "             (('now', 'EeEe'), 20.65371372356124),\n",
       "             (('But', 'I'), 20.62446272872406),\n",
       "             (('is', 'no'), 20.622692480864476),\n",
       "             (('his', 'chair'), 20.620258863252673),\n",
       "             (('SsSs', 'Your'), 20.394713394713392),\n",
       "             (('He', 'had'), 20.35694360218314),\n",
       "             (('with', 'you'), 20.319281991893458),\n",
       "             (('an', 'hour'), 20.318159806295398),\n",
       "             (('to', 'come'), 20.285137397813454),\n",
       "             (('all', 'EeEe'), 20.264098538823813),\n",
       "             (('we', 'are'), 20.144092219020173),\n",
       "             (('we', 'had'), 20.144092219020173),\n",
       "             (('and', 'yet'), 20.108419613205704),\n",
       "             (('and', 'so'), 20.108419613205704),\n",
       "             (('the', 'window'), 20.056231502795136),\n",
       "             (('the', 'morning'), 20.056231502795136),\n",
       "             (('When', 'I'), 19.85927029039464),\n",
       "             (('like', 'a'), 19.784468900704834),\n",
       "             (('We', 'have'), 19.758799555390887),\n",
       "             (('they', 'were'), 19.68217054263566),\n",
       "             (('know', 'that'), 19.66437177280551),\n",
       "             (('you', 'that'), 19.51621531260448),\n",
       "             (('to', 'say'), 19.496798975672213),\n",
       "             (('as', 'it'), 19.416),\n",
       "             (('as', 'you'), 19.416),\n",
       "             (('the', 'most'), 19.398224268332786),\n",
       "             (('it', 'to'), 19.37768430374142),\n",
       "             (('and', 'was'), 19.326626294198086),\n",
       "             (('had', 'not'), 19.300668664975788),\n",
       "             (('SsSs', 'One'), 19.206118206118205),\n",
       "             (('SsSs', 'he'), 19.206118206118205),\n",
       "             (('we', 'have'), 19.18299711815562),\n",
       "             (('so', 'I'), 19.16604679064187),\n",
       "             (('I', 'did'), 19.06982351239271),\n",
       "             (('hands', 'EeEe'), 18.848729076255424),\n",
       "             (('well', 'EeEe'), 18.83888957739497),\n",
       "             (('is', 'in'), 18.820801440792437),\n",
       "             (('of', 'her'), 18.807958820035637),\n",
       "             (('here', 'EeEe'), 18.787391841779975),\n",
       "             (('the', 'case'), 18.740217033870437),\n",
       "             (('the', 'first'), 18.740217033870437),\n",
       "             (('for', 'you'), 18.672752808988765),\n",
       "             (('There', 'are'), 18.670680756942737),\n",
       "             (('more', 'than'), 18.663432010809483),\n",
       "             (('you', 'may'), 18.624205951186894),\n",
       "             (('SsSs', 'At'), 18.61182061182061),\n",
       "             (('SsSs', 'On'), 18.61182061182061),\n",
       "             (('at', 'least'), 18.565126294959843),\n",
       "             (('which', 'had'), 18.524218840748055),\n",
       "             (('with', 'my'), 18.46543138390272),\n",
       "             (('with', 'me'), 18.46543138390272),\n",
       "             (('I', 'heard'), 18.29790722345453),\n",
       "             (('very', 'much'), 18.26122006978703),\n",
       "             (('this', 'morning'), 18.23576063446287),\n",
       "             (('Holmes', 'and'), 18.132305485837218),\n",
       "             (('said', 'that'), 18.045806067816777),\n",
       "             (('of', 'them'), 18.01564046723421),\n",
       "             (('among', 'the'), 17.938480697384808),\n",
       "             (('front', 'of'), 17.924340467894474),\n",
       "             (('Street', 'EeEe'), 17.884390910219793),\n",
       "             (('again', 'EeEe'), 17.87735849056604),\n",
       "             (('doubt', 'that'), 17.87735849056604),\n",
       "             (('me', 'and'), 17.83515707730321),\n",
       "             (('tell', 'me'), 17.82827782592547),\n",
       "             (('Yes', 'EeEe'), 17.80499567206628),\n",
       "             (('heard', 'of'), 17.79337617399901),\n",
       "             (('and', 'of'), 17.763039656182848),\n",
       "             (('and', 'in'), 17.763039656182848),\n",
       "             (('back', 'EeEe'), 17.75166461159063),\n",
       "             (('door', 'and'), 17.74473067915691),\n",
       "             (('Then', 'he'), 17.742420507764358),\n",
       "             (('not', 'be'), 17.735955056179776),\n",
       "             (('not', 'a'), 17.735955056179776),\n",
       "             (('not', 'know'), 17.735955056179776),\n",
       "             (('who', 'was'), 17.54359224484819),\n",
       "             (('with', 'her'), 17.538506079907354),\n",
       "             (('will', 'not'), 17.514243973703433),\n",
       "             (('what', 'you'), 17.478250303766707),\n",
       "             (('the', 'table'), 17.42420256494574),\n",
       "             (('on', 'a'), 17.395548566590058),\n",
       "             (('we', 'may'), 17.260806916426514),\n",
       "             (('of', 'us'), 17.223322114432786),\n",
       "             (('is', 'very'), 17.018910400720397),\n",
       "             (('that', 'his'), 17.017365205273876),\n",
       "             (('that', 'is'), 17.017365205273876),\n",
       "             (('in', 'front'), 17.007713734733233),\n",
       "             (('few', 'minutes'), 16.85500061965547),\n",
       "             (('he', 'might'), 16.85411554762436),\n",
       "             (('once', 'more'), 16.850576136786025),\n",
       "             (('As', 'I'), 16.841733746130032),\n",
       "             (('you', 'see'), 16.840187228351724),\n",
       "             (('you', 'think'), 16.840187228351724),\n",
       "             (('for', 'he'), 16.799157303370787),\n",
       "             (('the', 'time'), 16.76619533048339),\n",
       "             (('I', 'asked'), 16.754074645578164),\n",
       "             (('I', 'may'), 16.754074645578164),\n",
       "             (('I', 'see'), 16.754074645578164),\n",
       "             (('time', 'to'), 16.736304321063646),\n",
       "             (('it', 'in'), 16.71972548151428),\n",
       "             (('at', 'my'), 16.701780933535094),\n",
       "             (('at', 'his'), 16.701780933535094),\n",
       "             (('with', 'him'), 16.611580775911985),\n",
       "             (('when', 'you'), 16.54621848739496),\n",
       "             (('had', 'no'), 16.532395665206366),\n",
       "             (('was', 'very'), 16.4989068648885),\n",
       "             (('man', 'with'), 16.47386294724075),\n",
       "             (('one', 'EeEe'), 16.43579813627012),\n",
       "             (('have', 'had'), 16.423923006416132),\n",
       "             (('have', 'already'), 16.423923006416132),\n",
       "             (('to', 'get'), 16.343445287107258),\n",
       "             (('to', 'find'), 16.343445287107258),\n",
       "             (('to', 'your'), 16.343445287107258),\n",
       "             (('him', 'in'), 16.260333053791783),\n",
       "             (('SsSs', 'Why'), 16.234630234630234),\n",
       "             (('SsSs', 'Do'), 16.234630234630234),\n",
       "             (('that', 'they'), 16.15939543359417),\n",
       "             (('in', 'one'), 16.150203556888794),\n",
       "             (('said', 'EeEe'), 16.1412254610351),\n",
       "             (('said', 'the'), 16.1412254610351),\n",
       "             (('is', 'to'), 16.117964880684376),\n",
       "             (('he', 'asked'), 15.96140977024314),\n",
       "             (('me', 'in'), 15.95164136957289),\n",
       "             (('during', 'the'), 15.942846469929025),\n",
       "             (('answered', 'EeEe'), 15.900757669854677),\n",
       "             (('Miss', 'Hunter'), 15.86723689103756),\n",
       "             (('not', 'have'), 15.862359550561798),\n",
       "             (('for', 'some'), 15.862359550561798),\n",
       "             (('As', 'to'), 15.850526315789473),\n",
       "             (('way', 'EeEe'), 15.808894379246448),\n",
       "             (('face', 'EeEe'), 15.779874213836479),\n",
       "             (('know', 'what'), 15.728300958937792),\n",
       "             (('over', 'his'), 15.711864406779661),\n",
       "             (('SsSs', 'Sherlock'), 15.640332640332641),\n",
       "             (('SsSs', 'Mr'), 15.640332640332641),\n",
       "             (('of', 'our'), 15.638685408829936),\n",
       "             (('was', 'at'), 15.623961521644075),\n",
       "             (('when', 'she'), 15.571428571428571),\n",
       "             (('to', 'tell'), 15.555106864966021),\n",
       "             (('what', 'is'), 15.533171324422844),\n",
       "             (('have', 'not'), 15.506874427131072),\n",
       "             (('out', 'the'), 15.473123486682809),\n",
       "             (('the', 'whole'), 15.450180861558698),\n",
       "             (('the', 'very'), 15.450180861558698),\n",
       "             (('in', 'her'), 15.292693379044355),\n",
       "             (('Holmes', 'had'), 15.262459662961636),\n",
       "             (('upon', 'it'), 15.244985673352435),\n",
       "             (('is', 'an'), 15.217019360648356),\n",
       "             (('I', 'came'), 15.210242067701802),\n",
       "             (('I', 'EeEe'), 15.210242067701802),\n",
       "             (('he', 'remarked'), 15.068703992861924),\n",
       "             (('SsSs', 'Ah'), 15.046035046035046),\n",
       "             (('SsSs', 'For'), 15.046035046035046),\n",
       "             (('SsSs', 'Pray'), 15.046035046035046),\n",
       "             (('be', 'EeEe'), 15.006116927420303),\n",
       "             (('Quite', 'so'), 14.968079800498753),\n",
       "             (('began', 'to'), 14.96210893680668),\n",
       "             (('a', 'long'), 14.950577919489836),\n",
       "             (('a', 'good'), 14.950577919489836),\n",
       "             (('not', 'to'), 14.92556179775281),\n",
       "             (('between', 'the'), 14.920437593237196),\n",
       "             (('told', 'me'), 14.914502298993414),\n",
       "             (('end', 'of'), 14.912524850894632),\n",
       "             (('side', 'EeEe'), 14.869144981412639),\n",
       "             (('after', 'the'), 14.84950495049505),\n",
       "             (('at', 'all'), 14.838435572110349),\n",
       "             (('at', 'me'), 14.838435572110349),\n",
       "             (('hand', 'EeEe'), 14.824045471395033),\n",
       "             (('hand', 'and'), 14.824045471395033),\n",
       "             (('the', 'police'), 14.79217362709635),\n",
       "             (('as', 'far'), 14.776),\n",
       "             (('as', 'much'), 14.776),\n",
       "             (('time', 'EeEe'), 14.765603840945463),\n",
       "             (('before', 'I'), 14.738478554749907),\n",
       "             (('if', 'he'), 14.711446448288553),\n",
       "             (('had', 'come'), 14.686880332026746),\n",
       "             (('and', 'my'), 14.635866380152374),\n",
       "             (('my', 'companion'), 14.62922122674018),\n",
       "             (('have', 'done'), 14.589825847846013),\n",
       "             (('into', 'my'), 14.581457598247962),\n",
       "             (('You', 'see'), 14.575772318170761),\n",
       "             (('could', 'be'), 14.553072625698324),\n",
       "             (('He', 'is'), 14.532322619769557),\n",
       "             (('out', 'to'), 14.50411622276029),\n",
       "             (('are', 'the'), 14.490988266602153),\n",
       "             (('I', 'went'), 14.438325778763623),\n",
       "             (('been', 'a'), 14.384911100432484),\n",
       "             (('is', 'quite'), 14.316073840612336),\n",
       "             (('is', 'it'), 14.316073840612336),\n",
       "             (('but', 'it'), 14.280276816608996),\n",
       "             (('you', 'and'), 14.164159144098964),\n",
       "             (('the', 'man'), 14.134166392634002),\n",
       "             (('be', 'in'), 14.06458063757205),\n",
       "             (('it', 'out'), 14.061766659287137),\n",
       "             (('of', 'your'), 14.054048703227084),\n",
       "             (('not', 'EeEe'), 13.98876404494382),\n",
       "             (('not', 'think'), 13.98876404494382),\n",
       "             (('for', 'my'), 13.98876404494382),\n",
       "             (('to', 'know'), 13.978430020683541),\n",
       "             (('to', 'this'), 13.978430020683541),\n",
       "             (('seem', 'to'), 13.970074812967582),\n",
       "             (('believe', 'that'), 13.947709163346614),\n",
       "             (('across', 'the'), 13.94212818917237),\n",
       "             (('look', 'at'), 13.901328037731165),\n",
       "             (('thought', 'of'), 13.89023936500062),\n",
       "             (('young', 'lady'), 13.88285608032726),\n",
       "             (('find', 'it'), 13.879167183046226),\n",
       "             (('which', 'it'), 13.875595307236614),\n",
       "             (('which', 'has'), 13.875595307236614),\n",
       "             (('Watson', 'EeEe'), 13.875480113988353),\n",
       "             (('was', 'so'), 13.874070835155225),\n",
       "             (('SsSs', 'Very'), 13.857439857439857),\n",
       "             (('SsSs', 'So'), 13.857439857439857),\n",
       "             (('found', 'that'), 13.842378538756336),\n",
       "             (('What', 'do'), 13.81673454276194),\n",
       "             (('face', 'and'), 13.805771365149834),\n",
       "             (('am', 'sure'), 13.74214145383104),\n",
       "             (('And', 'yet'), 13.70962999264886),\n",
       "             (('I', 'felt'), 13.66640948982544),\n",
       "             (('Mr', 'Rucastle'), 13.595185995623632),\n",
       "             (('what', 'it'), 13.58809234507898),\n",
       "             (('in', 'your'), 13.577673023355475),\n",
       "             (('up', 'EeEe'), 13.543912780133253),\n",
       "             (('an', 'instant'), 13.535108958837773),\n",
       "             (('she', 'has'), 13.514023210831722),\n",
       "             (('the', 'way'), 13.476159158171654),\n",
       "             (('very', 'well'), 13.445915052340272),\n",
       "             (('is', 'of'), 13.415128320576317),\n",
       "             (('so', 'much'), 13.404319136172765),\n",
       "             (('her', 'EeEe'), 13.390507011866235),\n",
       "             (('him', 'that'), 13.383610878159818),\n",
       "             (('upon', 'her'), 13.333810888252149),\n",
       "             (('upon', 'me'), 13.333810888252149),\n",
       "             (('said', 'she'), 13.28435455086258),\n",
       "             (('you', 'not'), 13.272149782681378),\n",
       "             (('SsSs', 'Here'), 13.263142263142264),\n",
       "             (('of', 'an'), 13.261730350425658),\n",
       "             (('of', 'him'), 13.261730350425658),\n",
       "             (('of', 'you'), 13.261730350425658),\n",
       "             (('to', 'make'), 13.190091598542303),\n",
       "             (('it', 'up'), 13.175780385211423),\n",
       "             (('it', 'had'), 13.175780385211423),\n",
       "             (('it', 'all'), 13.175780385211423),\n",
       "             (('me', 'with'), 13.12636780797741),\n",
       "             (('and', 'down'), 13.072279742137136),\n",
       "             (('Neville', 'St'), 12.975555001247194),\n",
       "             (('appeared', 'to'), 12.961624719661101),\n",
       "             (('cry', 'of'), 12.958146487294469),\n",
       "             (('used', 'to'), 12.9546699875467),\n",
       "             (('which', 'were'), 12.945870600534324),\n",
       "             (('which', 'would'), 12.945870600534324),\n",
       "             (('which', 'she'), 12.945870600534324),\n",
       "             (('which', 'are'), 12.945870600534324),\n",
       "             (('let', 'me'), 12.942515864128406),\n",
       "             (('I', 'understand'), 12.894493200887261),\n",
       "             (('myself', 'EeEe'), 12.89238780064468),\n",
       "             (('She', 'is'), 12.86660066823413),\n",
       "             (('She', 'was'), 12.86660066823413),\n",
       "             (('morning', 'EeEe'), 12.861454725383474),\n",
       "             (('where', 'I'), 12.858026218154835),\n",
       "             (('had', 'the'), 12.84136499884713),\n",
       "             (('the', 'last'), 12.818151923709305),\n",
       "             (('room', 'and'), 12.810204584668474),\n",
       "             (('about', 'it'), 12.79490336082728),\n",
       "             (('my', 'dear'), 12.790489317711923),\n",
       "             (('am', 'afraid'), 12.759332023575638),\n",
       "             (('have', 'seen'), 12.755728689275895),\n",
       "             (('if', 'it'), 12.747515642252484),\n",
       "             (('should', 'not'), 12.735719539102721),\n",
       "             (('should', 'have'), 12.735719539102721),\n",
       "             (('And', 'what'), 12.728987993138936),\n",
       "             (('that', 'of'), 12.727516346875335),\n",
       "             (('in', 'an'), 12.720162845511034),\n",
       "             (('in', 'EeEe'), 12.720162845511034),\n",
       "             (('who', 'has'), 12.663699548835508),\n",
       "             (('do', 'so'), 12.630458581681062),\n",
       "             (('no', 'one'), 12.585838991270611),\n",
       "             (('out', 'EeEe'), 12.566101694915254),\n",
       "             (('an', 'old'), 12.566101694915254),\n",
       "             (('a', 'great'), 12.557991231566362),\n",
       "             (('a', 'woman'), 12.557991231566362),\n",
       "             (('she', 'would'), 12.54642166344294),\n",
       "             (('is', 'that'), 12.514182800540297),\n",
       "             (('is', 'my'), 12.514182800540297),\n",
       "             (('his', 'own'), 12.51266178953292),\n",
       "             (('of', 'that'), 12.469411997624233),\n",
       "             (('we', 'can'), 12.455331412103746),\n",
       "             (('to', 'think'), 12.401753176401064),\n",
       "             (('you', 'were'), 12.38014042126379),\n",
       "             (('you', 'could'), 12.38014042126379),\n",
       "             (('upon', 'a'), 12.378223495702006),\n",
       "             (('said', 'my'), 12.332064247471743),\n",
       "             (('it', 'has'), 12.28979411113571),\n",
       "             (('the', 'little'), 12.160144689246959),\n",
       "             (('was', 'only'), 12.124180148666376),\n",
       "             (('was', 'EeEe'), 12.124180148666376),\n",
       "             (('I', 'took'), 12.122576911949078),\n",
       "             (('I', 'answered'), 12.122576911949078),\n",
       "             (('SsSs', 'Its'), 12.074547074547075),\n",
       "             (('at', 'it'), 12.043417529973228),\n",
       "             (('Irene', 'Adler'), 11.980538922155688),\n",
       "             (('Thank', 'you'), 11.97730107258668),\n",
       "             (('Have', 'you'), 11.975682753460532),\n",
       "             (('afraid', 'that'), 11.972447325769854),\n",
       "             (('corner', 'of'), 11.954681274900398),\n",
       "             (('felt', 'that'), 11.953068592057761),\n",
       "             (('remarked', 'EeEe'), 11.919294760367519),\n",
       "             (('cried', 'EeEe'), 11.916087388282026),\n",
       "             (('head', 'EeEe'), 11.91448429936701),\n",
       "             (('business', 'EeEe'), 11.90967741935484),\n",
       "             (('nothing', 'of'), 11.885696594427245),\n",
       "             (('As', 'he'), 11.885696594427245),\n",
       "             (('Well', 'I'), 11.882505880896373),\n",
       "             (('right', 'EeEe'), 11.874536005939124),\n",
       "             (('my', 'mind'), 11.871123363197794),\n",
       "             (('my', 'wife'), 11.871123363197794),\n",
       "             (('that', 'a'), 11.869546575195626),\n",
       "             (('where', 'the'), 11.868167202572348),\n",
       "             (('Yes', 'sir'), 11.866575986150613),\n",
       "             (('away', 'from'), 11.863394733588825),\n",
       "             (('in', 'it'), 11.862652667666595),\n",
       "             (('heard', 'the'), 11.858625803262482),\n",
       "             (('have', 'ever'), 11.838680109990834),\n",
       "             (('have', 'heard'), 11.838680109990834),\n",
       "             (('back', 'in'), 11.830086313193588),\n",
       "             (('than', 'I'), 11.822181146025878),\n",
       "             (('come', 'EeEe'), 11.812707794606576),\n",
       "             (('before', 'he'), 11.7875138257343),\n",
       "             (('There', 'were'), 11.78594249201278),\n",
       "             (('see', 'EeEe'), 11.720293398533007),\n",
       "             (('see', 'the'), 11.720293398533007),\n",
       "             (('down', 'in'), 11.717184062576386),\n",
       "             (('of', 'some'), 11.677093644822808),\n",
       "             (('of', 'those'), 11.677093644822808),\n",
       "             (('will', 'find'), 11.66764061358656),\n",
       "             (('what', 'was'), 11.643013365735115),\n",
       "             (('man', 'and'), 11.620012128562765),\n",
       "             (('to', 'look'), 11.613414754259825),\n",
       "             (('to', 'EeEe'), 11.613414754259825),\n",
       "             (('out', 'upon'), 11.597094430992737),\n",
       "             (('on', 'my'), 11.586427966614249),\n",
       "             (('there', 'were'), 11.56059874456784),\n",
       "             (('the', 'name'), 11.50213745478461),\n",
       "             (('the', 'corner'), 11.50213745478461),\n",
       "             (('the', 'street'), 11.50213745478461),\n",
       "             (('the', 'fire'), 11.50213745478461),\n",
       "             (('the', 'old'), 11.50213745478461),\n",
       "             (('we', 'should'), 11.494236311239193),\n",
       "             (('you', 'in'), 11.488131059846205),\n",
       "             (('you', 'had'), 11.488131059846205),\n",
       "             (('SsSs', 'With'), 11.48024948024948),\n",
       "             (('Holmes', 'I'), 11.435998565794192),\n",
       "             (('but', 'you'), 11.415224913494809),\n",
       "             (('was', 'quite'), 11.24923480542195),\n",
       "             (('me', 'the'), 11.242852100247088),\n",
       "             (('me', 'as'), 11.242852100247088),\n",
       "             (('not', 'been'), 11.178370786516854),\n",
       "             (('which', 'we'), 11.086421187129748),\n",
       "             (('as', 'well'), 11.064),\n",
       "             (('as', 'if'), 11.064),\n",
       "             (('that', 'was'), 11.011576803515918),\n",
       "             (('had', 'left'), 10.995849665667512),\n",
       "             (('tonight', 'EeEe'), 10.973074046372476),\n",
       "             (('address', 'EeEe'), 10.968598130841121),\n",
       "             (('seems', 'to'), 10.965616045845271),\n",
       "             (('forward', 'and'), 10.959656331714607),\n",
       "             (('my', 'father'), 10.951757408683667),\n",
       "             (('my', 'hand'), 10.951757408683667),\n",
       "             (('fire', 'EeEe'), 10.949241104752426),\n",
       "             (('indeed', 'EeEe'), 10.943297687142502),\n",
       "             (('understand', 'that'), 10.940328194927897),\n",
       "             (('In', 'the'), 10.937360178970918),\n",
       "             (('chair', 'and'), 10.93587672424506),\n",
       "             (('though', 'I'), 10.928464977645305),\n",
       "             (('while', 'the'), 10.928464977645305),\n",
       "             (('name', 'of'), 10.926983732770395),\n",
       "             (('years', 'ago'), 10.925502855723863),\n",
       "             (('head', 'and'), 10.921062430184932),\n",
       "             (('thought', 'that'), 10.912191492000495),\n",
       "             (('until', 'he'), 10.909237445753254),\n",
       "             (('round', 'the'), 10.909237445753254),\n",
       "             (('young', 'man'), 10.906284864261808),\n",
       "             (('Watson', 'said'), 10.900384091190682),\n",
       "             (('day', 'and'), 10.900384091190682),\n",
       "             (('Oh', 'yes'), 10.900384091190682),\n",
       "             (('eyes', 'EeEe'), 10.895962348278424),\n",
       "             (('saw', 'the'), 10.894489164086687),\n",
       "             (('after', 'all'), 10.887128712871288),\n",
       "             (('SsSs', 'Of'), 10.885951885951886),\n",
       "             (('of', 'which'), 10.884775292021382),\n",
       "             (('No', 'EeEe'), 10.884187082405345),\n",
       "             (('No', 'no'), 10.884187082405345),\n",
       "             (('just', 'as'), 10.88124690747155),\n",
       "             (('made', 'a'), 10.870967741935484),\n",
       "             (('made', 'up'), 10.870967741935484),\n",
       "             (('heard', 'a'), 10.86950074147306),\n",
       "             (('door', 'of'), 10.83877727104647),\n",
       "             (('to', 'it'), 10.825076332118586),\n",
       "             (('they', 'are'), 10.818383167220377),\n",
       "             (('now', 'and'), 10.811116576487949),\n",
       "             (('know', 'EeEe'), 10.808212441603148),\n",
       "             (('think', 'of'), 10.808212441603148),\n",
       "             (('But', 'you'), 10.79516148839494),\n",
       "             (('But', 'the'), 10.79516148839494),\n",
       "             (('am', 'a'), 10.793713163064833),\n",
       "             (('shall', 'soon'), 10.793713163064833),\n",
       "             (('us', 'to'), 10.790817579179965),\n",
       "             (('us', 'and'), 10.790817579179965),\n",
       "             (('or', 'two'), 10.780694222985405),\n",
       "             (('see', 'him'), 10.741809290953546),\n",
       "             (('and', 'what'), 10.72689978511428),\n",
       "             (('is', 'all'), 10.712291760468258),\n",
       "             (('is', 'as'), 10.712291760468258),\n",
       "             (('his', 'father'), 10.710973550928532),\n",
       "             (('his', 'wife'), 10.710973550928532),\n",
       "             (('do', 'with'), 10.683250212869481),\n",
       "             (('You', 'may'), 10.68182923862807),\n",
       "             (('Mr', 'Sherlock'), 10.676148796498905),\n",
       "             (('could', 'have'), 10.664804469273744),\n",
       "             (('He', 'has'), 10.649241964827167),\n",
       "             (('man', 'of'), 10.649241964827167),\n",
       "             (('up', 'his'), 10.635130224106602),\n",
       "             (('by', 'his'), 10.635130224106602),\n",
       "             (('up', 'a'), 10.635130224106602),\n",
       "             (('are', 'not'), 10.618241199951616),\n",
       "             (('she', 'is'), 10.611218568665377),\n",
       "             (('were', 'to'), 10.608411892675852),\n",
       "             (('he', 'spoke'), 10.605175105955833),\n",
       "             (('there', 'and'), 10.594398841139546),\n",
       "             (('I', 'believe'), 10.578744334072717),\n",
       "             (('this', 'matter'), 10.541456380677722),\n",
       "             (('it', 'I'), 10.517821562984281),\n",
       "             (('it', 'as'), 10.517821562984281),\n",
       "             (('It', 'may'), 10.513785662910573),\n",
       "             (('It', 'must'), 10.513785662910573),\n",
       "             (('her', 'to'), 10.512405609492989),\n",
       "             (('was', 'still'), 10.374289462177526),\n",
       "             (('was', 'an'), 10.374289462177526),\n",
       "             (('be', 'so'), 10.298435478179037),\n",
       "             (('SsSs', 'To'), 10.291654291654291),\n",
       "             (('SsSs', 'By'), 10.291654291654291),\n",
       "             (('for', 'us'), 10.241573033707866),\n",
       "             (('not', 'tell'), 10.241573033707866),\n",
       "             (('for', 'him'), 10.241573033707866),\n",
       "             (('the', 'night'), 10.186122985859914),\n",
       "             (('the', 'only'), 10.186122985859914),\n",
       "             (('the', 'inspector'), 10.186122985859914),\n",
       "             (('the', 'lady'), 10.186122985859914),\n",
       "             (('at', 'one'), 10.180072168548481),\n",
       "             (('at', 'that'), 10.180072168548481),\n",
       "             (('a', 'most'), 10.165404543642886),\n",
       "             (('a', 'word'), 10.165404543642886),\n",
       "             (('which', 'the'), 10.15669648042746),\n",
       "             (('in', 'our'), 10.147632311977716),\n",
       "             (('in', 'that'), 10.147632311977716),\n",
       "             (('with', 'your'), 10.123103647944411),\n",
       "             (('of', 'such'), 10.092456939219957),\n",
       "             (('my', 'room'), 10.032391454169538),\n",
       "             (('have', 'to'), 10.004582951420716),\n",
       "             (('have', 'come'), 10.004582951420716),\n",
       "             (('have', 'made'), 10.004582951420716),\n",
       "             (('Stoke', 'Moran'), 9.986274020464188),\n",
       "             (('number', 'of'), 9.984903306300685),\n",
       "             (('signs', 'of'), 9.983532934131736),\n",
       "             (('Ha', 'EeEe'), 9.983532934131736),\n",
       "             (('account', 'of'), 9.979423868312757),\n",
       "             (('Hosmer', 'Angel'), 9.979423868312757),\n",
       "             (('appears', 'to'), 9.976686198728338),\n",
       "             (('photograph', 'EeEe'), 9.97258225324028),\n",
       "             (('street', 'EeEe'), 9.97258225324028),\n",
       "             (('floor', 'EeEe'), 9.968481375358166),\n",
       "             (('fear', 'that'), 9.965749159297546),\n",
       "             (('alone', 'EeEe'), 9.964383561643835),\n",
       "             (('part', 'of'), 9.964383561643835),\n",
       "             (('whom', 'I'), 9.963018304071722),\n",
       "             (('became', 'a'), 9.96028880866426),\n",
       "             (('together', 'EeEe'), 9.958924570575055),\n",
       "             (('less', 'than'), 9.958924570575055),\n",
       "             (('table', 'EeEe'), 9.957560672059738),\n",
       "             (('London', 'EeEe'), 9.957560672059738),\n",
       "             (('sure', 'that'), 9.954833893243748),\n",
       "             (('One', 'of'), 9.954833893243748),\n",
       "             (('hat', 'and'), 9.953471012689723),\n",
       "             (('money', 'EeEe'), 9.952108471202886),\n",
       "             (('Mrs', 'Rucastle'), 9.950746268656717),\n",
       "             (('gave', 'a'), 9.948022879880627),\n",
       "             (('minutes', 'EeEe'), 9.948022879880627),\n",
       "             (('place', 'EeEe'), 9.948022879880627),\n",
       "             (('and', 'she'), 9.945106466106662),\n",
       "             (('woman', 'EeEe'), 9.941220330557972),\n",
       "             (('told', 'you'), 9.941220330557972),\n",
       "             (('end', 'EeEe'), 9.939860834990059),\n",
       "             (('while', 'I'), 9.934426229508198),\n",
       "             (('open', 'and'), 9.933068421706196),\n",
       "             (('name', 'is'), 9.933068421706196),\n",
       "             (('open', 'EeEe'), 9.933068421706196),\n",
       "             (('went', 'to'), 9.92221808708597),\n",
       "             (('If', 'I'), 9.920863309352518),\n",
       "             (('too', 'much'), 9.91950886766712),\n",
       "             (('take', 'the'), 9.918154761904763),\n",
       "             (('round', 'and'), 9.916800991940484),\n",
       "             (('yet', 'I'), 9.908685416924792),\n",
       "             (('eyes', 'and'), 9.904632152588556),\n",
       "             (('saw', 'him'), 9.90328173374613),\n",
       "             (('asked', 'Holmes'), 9.897883401411066),\n",
       "             (('go', 'to'), 9.897883401411066),\n",
       "             (('good', 'EeEe'), 9.895186239326817),\n",
       "             (('No', 'I'), 9.893838158871567),\n",
       "             (('case', 'EeEe'), 9.892490411975752),\n",
       "             (('away', 'EeEe'), 9.884410928421312),\n",
       "             (('way', 'to'), 9.876343421865348),\n",
       "             (('We', 'shall'), 9.873656909966654),\n",
       "             (('other', 'side'), 9.848201084277969),\n",
       "             (('come', 'from'), 9.841521980051718),\n",
       "             (('time', 'that'), 9.838852640650007),\n",
       "             (('before', 'the'), 9.820204006390561),\n",
       "             (('more', 'EeEe'), 9.814887605945216),\n",
       "             (('But', 'what'), 9.81223136436203),\n",
       "             (('is', 'so'), 9.811346240432238),\n",
       "             (('am', 'not'), 9.81090373280943),\n",
       "             (('his', 'long'), 9.810129431626336),\n",
       "             (('his', 'son'), 9.810129431626336),\n",
       "             (('us', 'in'), 9.808249447581636),\n",
       "             (('I', 'say'), 9.806828045134536),\n",
       "             (('should', 'like'), 9.792351066437853),\n",
       "             (('can', 'hardly'), 9.777818582445832),\n",
       "             (('can', 'be'), 9.777818582445832),\n",
       "             (('down', 'EeEe'), 9.760694206795405),\n",
       "             (('some', 'time'), 9.74755859375),\n",
       "             (('will', 'do'), 9.718772826880935),\n",
       "             (('he', 'cried'), 9.712469328574615),\n",
       "             (('he', 'answered'), 9.712469328574615),\n",
       "             (('do', 'it'), 9.709646028463691),\n",
       "             (('You', 'must'), 9.708343468742399),\n",
       "             (('You', 'can'), 9.708343468742399),\n",
       "             (('Mr', 'Holder'), 9.703136396790663),\n",
       "             (('Mr', 'Windibank'), 9.703136396790663),\n",
       "             (('Mr', 'EeEe'), 9.703136396790663),\n",
       "             (('what', 'he'), 9.697934386391251),\n",
       "             (('SsSs', 'asked'), 9.697356697356696),\n",
       "             (('SsSs', 'All'), 9.697356697356696),\n",
       "             (('SsSs', 'From'), 9.697356697356696),\n",
       "             (('no', 'more'), 9.67458777885548),\n",
       "             (('one', 'to'), 9.655209972165073),\n",
       "             (('one', 'and'), 9.655209972165073),\n",
       "             (('one', 'who'), 9.655209972165073),\n",
       "             (('on', 'his'), 9.65005443328898),\n",
       "             (('are', 'EeEe'), 9.65005443328898),\n",
       "             (('were', 'all'), 9.641044234952865),\n",
       "             (('were', 'not'), 9.641044234952865),\n",
       "             (('it', 'may'), 9.631835288908567),\n",
       "             (('this', 'man'), 9.579668348954579),\n",
       "             (('this', 'is'), 9.579668348954579),\n",
       "             (('her', 'but'), 9.553038475368572),\n",
       "             (('him', 'for'), 9.547981310650533),\n",
       "             (('the', 'coronet'), 9.528115751397568),\n",
       "             (('the', 'facts'), 9.528115751397568),\n",
       "             (('the', 'two'), 9.528115751397568),\n",
       "             (('Holmes', 'as'), 9.52276801721047),\n",
       "             (('Holmes', 'that'), 9.52276801721047),\n",
       "             (('from', 'my'), 9.52151051625239),\n",
       "             (('but', 'there'), 9.505190311418685),\n",
       "             (('was', 'that'), 9.4993441189331),\n",
       "             (('be', 'able'), 9.356899188330784),\n",
       "             (('be', 'of'), 9.356899188330784),\n",
       "             (('be', 'no'), 9.356899188330784),\n",
       "             (('for', 'his'), 9.304775280898877),\n",
       "             (('not', 'the'), 9.304775280898877),\n",
       "             (('of', 'these'), 9.30013858641853),\n",
       "             (('that', 'if'), 9.295637260156502),\n",
       "             (('that', 'in'), 9.295637260156502),\n",
       "             (('that', 'her'), 9.295637260156502),\n",
       "             (('in', 'London'), 9.290122134133277),\n",
       "             (('in', 'some'), 9.290122134133277),\n",
       "             (('to', 'put'), 9.248399487836107),\n",
       "             (('to', 'give'), 9.248399487836107),\n",
       "             (('which', 'have'), 9.226971773725172),\n",
       "             (('with', 'which'), 9.196178343949045),\n",
       "             (('with', 'it'), 9.196178343949045),\n",
       "             (('and', 'had'), 9.163313147099043),\n",
       "             (('had', 'gone'), 9.150334332487894),\n",
       "             (('had', 'to'), 9.150334332487894),\n",
       "             (('had', 'an'), 9.150334332487894),\n",
       "             (('SsSs', 'Quite'), 9.103059103059103),\n",
       "             (('SsSs', 'Our'), 9.103059103059103),\n",
       "             (('SsSs', 'Not'), 9.103059103059103),\n",
       "             (('have', 'you'), 9.087534372135655),\n",
       "             (('have', 'it'), 9.087534372135655),\n",
       "             (('I', 'remarked'), 9.034911756196355),\n",
       "             (('I', 'suppose'), 9.034911756196355),\n",
       "             (('Briony', 'Lodge'), 8.988768251591164),\n",
       "             (('connection', 'with'), 8.986275733000625),\n",
       "             (('tried', 'to'), 8.983784458026694),\n",
       "             (('difficult', 'to'), 8.982539286605139),\n",
       "             (('cab', 'and'), 8.975074775672981),\n",
       "             (('With', 'a'), 8.975074775672981),\n",
       "             (('Of', 'course'), 8.975074775672981),\n",
       "             (('companion', 'EeEe'), 8.972589085472215),\n",
       "             (('air', 'of'), 8.972589085472215),\n",
       "             (('fact', 'that'), 8.97134670487106),\n",
       "             (('each', 'other'), 8.970104633781764),\n",
       "             (('hope', 'that'), 8.968862872088678),\n",
       "             (('either', 'side'), 8.967621419676215),\n",
       "             (('want', 'to'), 8.966380276428838),\n",
       "             (('crime', 'EeEe'), 8.965139442231076),\n",
       "             (('save', 'the'), 8.961418792781581),\n",
       "             (('point', 'EeEe'), 8.96017919362867),\n",
       "             (('At', 'the'), 8.96017919362867),\n",
       "             (('interest', 'EeEe'), 8.96017919362867),\n",
       "             (('papers', 'EeEe'), 8.955223880597014),\n",
       "             (('Mrs', 'St'), 8.955223880597014),\n",
       "             (('within', 'a'), 8.953985822658874),\n",
       "             (('far', 'from'), 8.944092433842714),\n",
       "             (('open', 'the'), 8.939153110641996),\n",
       "             (('remarked', 'Holmes'), 8.937919046436553),\n",
       "             (('cried', 'the'), 8.93545183714002),\n",
       "             (('friend', 'and'), 8.930521091811414),\n",
       "             (('without', 'a'), 8.930521091811414),\n",
       "             (('too', 'EeEe'), 8.926826243333746),\n",
       "             (('round', 'to'), 8.924364538127712),\n",
       "             (('Miss', 'Stoner'), 8.921904053551506),\n",
       "             (('took', 'a'), 8.920674268715915),\n",
       "             (('My', 'dear'), 8.9182156133829),\n",
       "             (('day', 'EeEe'), 8.916986742658903),\n",
       "             (('yet', 'EeEe'), 8.916986742658903),\n",
       "             (('last', 'night'), 8.909619908381824),\n",
       "             (('his', 'room'), 8.909285312324142),\n",
       "             (('quite', 'a'), 8.907166728555515),\n",
       "             (('good', 'enough'), 8.904714763024378),\n",
       "             (('That', 'was'), 8.899814471243042),\n",
       "             (('where', 'he'), 8.898590155824882),\n",
       "             (('however', 'and'), 8.897366143192778),\n",
       "             (('like', 'to'), 8.897366143192778),\n",
       "             (('Yes', 'it'), 8.897366143192778),\n",
       "             (('Yes', 'I'), 8.897366143192778),\n",
       "             (('away', 'to'), 8.894919025837558),\n",
       "             (('here', 'and'), 8.893695920889988),\n",
       "             (('way', 'in'), 8.887584928968499),\n",
       "             (('the', 'country'), 8.870108516935218),\n",
       "             (('the', 'windows'), 8.870108516935218),\n",
       "             (('the', 'day'), 8.870108516935218),\n",
       "             (('came', 'in'), 8.866863905325443),\n",
       "             (('than', 'the'), 8.863216266173753),\n",
       "             (('other', 'EeEe'), 8.862000985707246),\n",
       "             (('come', 'back'), 8.85592907277429),\n",
       "             (('you', 'as'), 8.812102975593447),\n",
       "             (('you', 'said'), 8.812102975593447),\n",
       "             (('And', 'you'), 8.80641999509924),\n",
       "             (('down', 'into'), 8.782449278904913),\n",
       "             (('down', 'upon'), 8.782449278904913),\n",
       "             (('down', 'a'), 8.782449278904913),\n",
       "             (('then', 'I'), 8.771700647051642),\n",
       "             (('some', 'little'), 8.7705078125),\n",
       "             (('who', 'have'), 8.759785392025362),\n",
       "             (('it', 'said'), 8.745849014832853),\n",
       "             (('it', 'seemed'), 8.745849014832853),\n",
       "             (('it', 'from'), 8.745849014832853),\n",
       "             (('it', 'over'), 8.745849014832853),\n",
       "             (('will', 'excuse'), 8.744338933528123),\n",
       "             (('do', 'nothing'), 8.736041844057901),\n",
       "             (('Mr', 'Merryweather'), 8.730123997082421),\n",
       "             (('has', 'a'), 8.730123997082421),\n",
       "             (('has', 'not'), 8.730123997082421),\n",
       "             (('could', 'hardly'), 8.720670391061452),\n",
       "             (('out', 'into'), 8.690072639225182),\n",
       "             (('out', 'in'), 8.690072639225182),\n",
       "             (('out', 'his'), 8.690072639225182),\n",
       "             (('one', 'or'), 8.686554520150066),\n",
       "             (('one', 'side'), 8.686554520150066),\n",
       "             (('are', 'you'), 8.681867666626346),\n",
       "             (('are', 'very'), 8.681867666626346),\n",
       "             (('she', 'cried'), 8.676015473887814),\n",
       "             (('The', 'man'), 8.661999034282955),\n",
       "             (('was', 'to'), 8.624398775688675),\n",
       "             (('was', 'as'), 8.624398775688675),\n",
       "             (('this', 'EeEe'), 8.617880317231435),\n",
       "             (('we', 'could'), 8.610951008645532),\n",
       "             (('so', 'as'), 8.602879424115176),\n",
       "             (('a', 'cab'), 8.570346751693902),\n",
       "             (('from', 'which'), 8.565009560229445),\n",
       "             (('but', 'she'), 8.550173010380623),\n",
       "             (('but', 'when'), 8.550173010380623),\n",
       "             (('SsSs', 'Where'), 8.50876150876151),\n",
       "             (('SsSs', 'Let'), 8.50876150876151),\n",
       "             (('to', 'take'), 8.46006106569487),\n",
       "             (('to', 'ask'), 8.46006106569487),\n",
       "             (('me', 'for'), 8.417578538651606),\n",
       "             (('be', 'at'), 8.415362898482531),\n",
       "             (('be', 'some'), 8.415362898482531),\n",
       "             (('and', 'you'), 8.381519828091424),\n",
       "             (('and', 'looked'), 8.381519828091424),\n",
       "             (('not', 'say'), 8.367977528089888),\n",
       "             (('for', 'all'), 8.367977528089888),\n",
       "             (('not', 'very'), 8.367977528089888),\n",
       "             (('at', 'Baker'), 8.316726807123734),\n",
       "             (('at', 'this'), 8.316726807123734),\n",
       "             (('which', 'may'), 8.297247067022884),\n",
       "             (('as', 'she'), 8.28),\n",
       "             (('I', 'fear'), 8.262995467258174),\n",
       "             ...])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "OrderedDict(sorted(count_bigrams_diff.items(), key=lambda t: t[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above output cell gives an Ordered Dictionary which contains bigrams with maximum count difference after add 1 smoothing. From the top bigrams, it can be concluded that bigrams whise frequency is more in the training corpus appears to have more count difference after add 1 smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_bigrams_values = np.array(list(count_bigrams.values()))\n",
    "good_turing_N = dict()\n",
    "good_turing_count_smoothed = dict()\n",
    "num = 11\n",
    "for i in range(1,num+1):\n",
    "    good_turing_N[i] = np.where(count_bigrams_values==i)[0].shape[0]\n",
    "\n",
    "for i in range(1,num):\n",
    "    good_turing_count_smoothed[i] = ((i + 1) * good_turing_N[i+1])/good_turing_N[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_turing_count_diff=dict()\n",
    "for k, v in good_turing_count_smoothed.items():\n",
    "    good_turing_count_diff[k] = k - v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.29913616800463966,\n",
       " 2: 1.0946938775510204,\n",
       " 3: 2.1297539149888145,\n",
       " 4: 2.8256302521008405,\n",
       " 5: 4.014869888475836,\n",
       " 6: 5.347222222222222,\n",
       " 7: 4.712727272727273,\n",
       " 8: 7.333333333333333,\n",
       " 9: 10.454545454545455,\n",
       " 10: 8.289855072463768}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_turing_count_smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 32761,\n",
       " 2: 4900,\n",
       " 3: 1788,\n",
       " 4: 952,\n",
       " 5: 538,\n",
       " 6: 360,\n",
       " 7: 275,\n",
       " 8: 162,\n",
       " 9: 132,\n",
       " 10: 138,\n",
       " 11: 104}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_turing_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.7008638319953604,\n",
       " 2: 0.9053061224489796,\n",
       " 3: 0.8702460850111855,\n",
       " 4: 1.1743697478991595,\n",
       " 5: 0.985130111524164,\n",
       " 6: 0.6527777777777777,\n",
       " 7: 2.287272727272727,\n",
       " 8: 0.666666666666667,\n",
       " 9: -1.454545454545455,\n",
       " 10: 1.7101449275362324}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_turing_count_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniq_tokens = sorted(set(tokens),key = tokens.index)\n",
    "# voc_len = len(uniq_tokens)\n",
    "# count_bigrams_smoothed = dict() \n",
    "# for i in range(len(uniq_tokens)):\n",
    "#     for j in range(len(uniq_tokens)):\n",
    "#         if (uniq_tokens[i],uniq_tokens[j]) in prob_bigrams:\n",
    "#             count_bigrams_smoothed[(uniq_tokens[i],uniq_tokens[j])] = ((count_bigrams[(uniq_tokens[i],uniq_tokens[j])] + 1) * count_unigrams[(uniq_tokens[i],)])/(count_unigrams[(uniq_tokens[i],)] + voc_len)\n",
    "#         else:\n",
    "#             count_bigrams_smoothed[(uniq_tokens[i],uniq_tokens[j])] = count_unigrams[(uniq_tokens[i],)]/(count_unigrams[(uniq_tokens[i],)] + voc_len)\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod_text_og = \"\".join(sent_tokenize_filtered_list_og)\n",
    "# # tokens = sorted(set(word_tokenize_list),key = word_tokenize_list.index)\n",
    "# tokens_og = word_tokenize(mod_text_og)\n",
    "# uniq_tokens = sorted(set(tokens_og),key = tokens_og.index)\n",
    "# voc_len = len(uniq_tokens)\n",
    "# prob_bigrams_add1_smoothed = dict() \n",
    "# for i in range(len(uniq_tokens)):\n",
    "#     for j in range(len(uniq_tokens)):\n",
    "#         if (uniq_tokens[i],uniq_tokens[j]) in prob_bigrams:\n",
    "#             prob_bigrams_add1_smoothed[(uniq_tokens[i],uniq_tokens[j])] = (count_bigrams[(uniq_tokens[i],uniq_tokens[j])] + 1)/(count_unigrams[(uniq_tokens[i],)] + voc_len)\n",
    "#         else:\n",
    "#             prob_bigrams_add1_smoothed[(uniq_tokens[i],uniq_tokens[j])] = 1/(count_unigrams[(uniq_tokens[i],)] + voc_len)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_text_og = \"\".join(sent_tokenize_filtered_list_og)\n",
    "# tokens = sorted(set(word_tokenize_list),key = word_tokenize_list.index)\n",
    "tokens_og = word_tokenize(mod_text_og)\n",
    "uniq_tokens = sorted(set(tokens_og),key = tokens_og.index)\n",
    "voc_len = len(uniq_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_prob_add1_smoothed = 1\n",
    "perplexity_add1 = []\n",
    "for i in range(len(sent_tokenize_filtered_list_test)):\n",
    "    words = word_tokenize(sent_tokenize_filtered_list_test[i])\n",
    "    words = [words[k].translate(translate_table_3) for k in range(len(words))]\n",
    "    for z in range(len(words)):\n",
    "        if words[z].isalpha()==False:\n",
    "            words[z]=\"\"\n",
    "    words = [value for value in words if value != \"\"]\n",
    "    res_prob = 1\n",
    "    for j in range(len(words)-1):\n",
    "        if (words[j],words[j+1]) in prob_bigrams:\n",
    "            int_prob = (count_bigrams[(words[j],words[j+1])] + 1)/(count_unigrams[(words[j],)] + voc_len)\n",
    "        else:\n",
    "            int_prob = 1/(count_unigrams[(words[j],)] + voc_len)\n",
    "        res_prob = res_prob * int_prob\n",
    "        print(res_prob,int_prob)\n",
    "    perp = ((1.0/res_prob)**(1.0/len(words)))\n",
    "#     result_prob_add1_smoothed = result_prob_add1_smoothed * probab\n",
    "    perplexity_add1 = perplexity_add1 + [perp] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics as sts\n",
    "perplexity_add1_mean = sts.mean(perplexity_add1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1169.1780768686785"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity_add1_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_bigrams_values = np.array(list(count_bigrams.values()))\n",
    "good_turing_N = dict()\n",
    "good_turing_count_smoothed = dict()\n",
    "for i in set(count_bigrams_values):\n",
    "    good_turing_N[i] = np.where(count_bigrams_values==i)[0].shape[0]\n",
    "\n",
    "# # for i in range(1,11):\n",
    "# #     good_turing_count_smoothed[i] = ((i + 1) * good_turing_N[i+1])/good_turing_N[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 32761,\n",
       " 2: 4900,\n",
       " 3: 1788,\n",
       " 4: 952,\n",
       " 5: 538,\n",
       " 6: 360,\n",
       " 7: 275,\n",
       " 8: 162,\n",
       " 9: 132,\n",
       " 10: 138,\n",
       " 11: 104,\n",
       " 12: 73,\n",
       " 13: 48,\n",
       " 14: 55,\n",
       " 15: 43,\n",
       " 16: 41,\n",
       " 17: 25,\n",
       " 18: 38,\n",
       " 19: 28,\n",
       " 20: 21,\n",
       " 21: 25,\n",
       " 22: 17,\n",
       " 23: 18,\n",
       " 24: 18,\n",
       " 25: 12,\n",
       " 26: 18,\n",
       " 27: 19,\n",
       " 28: 20,\n",
       " 29: 9,\n",
       " 30: 6,\n",
       " 31: 7,\n",
       " 32: 14,\n",
       " 33: 8,\n",
       " 34: 3,\n",
       " 35: 6,\n",
       " 36: 6,\n",
       " 37: 7,\n",
       " 38: 5,\n",
       " 39: 5,\n",
       " 40: 2,\n",
       " 41: 5,\n",
       " 42: 4,\n",
       " 43: 3,\n",
       " 44: 4,\n",
       " 45: 5,\n",
       " 46: 3,\n",
       " 47: 1,\n",
       " 48: 3,\n",
       " 49: 1,\n",
       " 50: 3,\n",
       " 563: 1,\n",
       " 52: 4,\n",
       " 53: 4,\n",
       " 54: 2,\n",
       " 55: 1,\n",
       " 56: 4,\n",
       " 57: 1,\n",
       " 58: 2,\n",
       " 60: 1,\n",
       " 61: 1,\n",
       " 62: 2,\n",
       " 63: 2,\n",
       " 64: 3,\n",
       " 65: 1,\n",
       " 66: 2,\n",
       " 67: 1,\n",
       " 70: 1,\n",
       " 71: 3,\n",
       " 72: 2,\n",
       " 74: 2,\n",
       " 76: 3,\n",
       " 77: 3,\n",
       " 78: 2,\n",
       " 79: 2,\n",
       " 83: 2,\n",
       " 84: 1,\n",
       " 85: 2,\n",
       " 87: 2,\n",
       " 88: 2,\n",
       " 91: 1,\n",
       " 92: 2,\n",
       " 93: 1,\n",
       " 96: 3,\n",
       " 100: 3,\n",
       " 102: 1,\n",
       " 103: 2,\n",
       " 105: 2,\n",
       " 106: 1,\n",
       " 110: 1,\n",
       " 117: 1,\n",
       " 118: 1,\n",
       " 119: 1,\n",
       " 120: 1,\n",
       " 126: 2,\n",
       " 128: 1,\n",
       " 131: 1,\n",
       " 133: 1,\n",
       " 134: 1,\n",
       " 137: 1,\n",
       " 138: 2,\n",
       " 139: 1,\n",
       " 144: 1,\n",
       " 147: 2,\n",
       " 152: 1,\n",
       " 158: 1,\n",
       " 159: 1,\n",
       " 162: 1,\n",
       " 180: 1,\n",
       " 194: 1,\n",
       " 218: 1,\n",
       " 236: 1,\n",
       " 239: 1,\n",
       " 251: 1,\n",
       " 268: 1,\n",
       " 796: 1,\n",
       " 335: 1,\n",
       " 5463: 1,\n",
       " 397: 1}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_turing_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 1\n",
    "i = 10\n",
    "while (d >= 1.0 or d < 0.0):\n",
    "    d = sts.median(list(good_turing_count_diff.values())[:i])\n",
    "    i = i-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8877761037300825"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_text_og = \"\".join(sent_tokenize_filtered_list_og)\n",
    "# tokens = sorted(set(word_tokenize_list),key = word_tokenize_list.index)\n",
    "tokens_og = word_tokenize(mod_text_og)\n",
    "translate_table_3 = dict((ord(char), None) for char in string.punctuation[0:17] or string.punctuation[18] or string.punctuation[20:])\n",
    "tokens_og = [tokens_og[i].translate(translate_table_3) for i in range(len(tokens_og))]\n",
    "for i in range(len(tokens_og)):\n",
    "    if tokens_og[i].isalpha()==False:\n",
    "        tokens_og[i]=\"\"\n",
    "tokens_og = [value for value in tokens_og if value != \"\"]\n",
    "# for i in range(len(tokens)):\n",
    "#    if tokens[i].isalpha()==False:\n",
    "#        tokens[i]=\"\"\n",
    "# tokens = sorted(set(tokens),key = tokens.index)\n",
    "\n",
    "from collections import Counter\n",
    "unigrams_og = list(nltk.ngrams(tokens_og, 1))\n",
    "bigrams_og = list(nltk.ngrams(tokens_og, 2))\n",
    "trigrams_og = list(nltk.ngrams(tokens_og, 3))\n",
    "quadgrams_og = list(nltk.ngrams(tokens_og, 4))\n",
    "\n",
    "count_unigrams_og = Counter(unigrams_og)\n",
    "count_bigrams_og = Counter(bigrams_og)\n",
    "count_trigrams_og = Counter(trigrams_og)\n",
    "count_quadgrams_og = Counter(quadgrams_og)\n",
    "no_of_tokens_og = len(tokens_og)\n",
    "\n",
    "prob_unigrams_og = dict()\n",
    "for i in range(len(unigrams_og)):\n",
    "    prob_unigrams_og[unigrams_og[i]] = count_unigrams_og[unigrams_og[i]]/no_of_tokens_og \n",
    "    \n",
    "prob_bigrams_og = dict()\n",
    "for i in range(len(bigrams_og)):\n",
    "    prob_bigrams_og[bigrams_og[i]] = count_bigrams_og[bigrams_og[i]]/count_unigrams_og[(bigrams_og[i][0],)] \n",
    "\n",
    "prob_trigrams_og = dict()\n",
    "for i in range(len(trigrams_og)):\n",
    "    prob_trigrams_og[trigrams_og[i]] = count_trigrams_og[trigrams_og[i]]/count_bigrams_og[(trigrams_og[i][0],trigrams_og[i][1])] \n",
    "    \n",
    "prob_quadgrams_og = dict()\n",
    "for i in range(len(quadgrams_og)):\n",
    "    prob_quadgrams_og[quadgrams_og[i]] = count_quadgrams_og[quadgrams_og[i]]/count_trigrams_og[(quadgrams_og[i][0],quadgrams_og[i][1],quadgrams_og[i][2])] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_prob_add1_smoothed = 1\n",
    "perplexity_good_turing = []\n",
    "for i in range(len(sent_tokenize_filtered_list_test)):\n",
    "    words = word_tokenize(sent_tokenize_filtered_list_test[i])\n",
    "    words = [words[k].translate(translate_table_3) for k in range(len(words))]\n",
    "    for z in range(len(words)):\n",
    "        if words[z].isalpha()==False:\n",
    "            words[z]=\"\"\n",
    "    words = [value for value in words if value != \"\"]\n",
    "    res_prob = 1\n",
    "    for j in range(len(words)-1):\n",
    "        if (words[j],words[j+1]) in prob_bigrams:\n",
    "            int_prob = (count_bigrams[(words[j],words[j+1])] - d)/(count_unigrams[(words[j],)])\n",
    "        else:\n",
    "            int_prob = (d/count_unigrams_og[(words[j],)])*prob_unigrams_og[(words[j+1],)]\n",
    "        res_prob = res_prob * int_prob\n",
    "        print(res_prob,int_prob)\n",
    "    perp = ((1.0/res_prob)**(1.0/len(words)))\n",
    "#     result_prob_add1_smoothed = result_prob_add1_smoothed * probab\n",
    "    perplexity_good_turing = perplexity_good_turing + [perp] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics as sts\n",
    "perplexity_good_turing_mean = sts.mean(perplexity_good_turing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1052.5785510999842"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity_good_turing_mean"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Comparing the perplexity values, it can be concluded that Good Turing Smoothing is relatively better than Add 1 Smoothing technique\n",
    "Also which smoothing technique performs better depend on the training data choosen and also the d value which is choosen, hence in one of the trails of the code it might occur that Add 1 performs relatively better than Good Turing Smoothing Technique\n",
    "\n",
    "Note: Sometimes due to the random selection of training and test set, the perplexity values might go to inf. If that happens just change the training and test datasets by running train_test_split function again and the respective consecutive codes. If there was some way which would allow perplexity not going to inf pleae let me know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
